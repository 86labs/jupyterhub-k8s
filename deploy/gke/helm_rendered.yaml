---
# Source: jupyterhub/templates/hub/netpol.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  podSelector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: jupyterhub
  policyTypes:
    - Ingress
    - Egress

  # IMPORTANT:
  # NetworkPolicy's ingress "from" and egress "to" rule specifications require
  # great attention to detail. A quick summary is:
  #
  # 1. You can provide "from"/"to" rules that provide access either ports or a
  #    subset of ports.
  # 2. You can for each "from"/"to" rule provide any number of
  #    "sources"/"destinations" of four different kinds.
  #    - podSelector                        - targets pods with a certain label in the same namespace as the NetworkPolicy
  #    - namespaceSelector                  - targets all pods running in namespaces with a certain label
  #    - namespaceSelector and podSelector  - targets pods with a certain label running in namespaces with a certain label
  #    - ipBlock                            - targets network traffic from/to a set of IP address ranges
  #
  # Read more at: https://kubernetes.io/docs/concepts/services-networking/network-policies/#behavior-of-to-and-from-selectors
  #
  ingress:

    # allowed pods (hub.jupyter.org/network-access-hub) --> hub
    - ports:
        - port: http
      from:
        # source 1 - labeled pods
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-hub: "true"

  egress:
    # hub --> proxy
    - to:
        - podSelector:
            matchLabels:
              component: proxy
              app: jupyterhub
              release: jupyterhub
      ports:
        - port: 8001

    # hub --> singleuser-server
    - to:
        - podSelector:
            matchLabels:
              component: singleuser-server
              app: jupyterhub
              release: jupyterhub
      ports:
        - port: 8888
    
    # Allow outbound connections to the DNS port in the private IP ranges
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
      to:
        - ipBlock:
            cidr: 10.0.0.0/8
        - ipBlock:
            cidr: 172.16.0.0/12
        - ipBlock:
            cidr: 192.168.0.0/16
    # Allow outbound connections to non-private IP ranges
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              # As part of this rule, don't:
              # - allow outbound connections to private IP
              - 10.0.0.0/8
              - 172.16.0.0/12
              - 192.168.0.0/16
              # - allow outbound connections to the cloud metadata server
              - 169.254.169.254/32
    # Allow outbound connections to private IP ranges
    - to:
        - ipBlock:
            cidr: 10.0.0.0/8
        - ipBlock:
            cidr: 172.16.0.0/12
        - ipBlock:
            cidr: 192.168.0.0/16
    # Allow outbound connections to the cloud metadata server
    - to:
        - ipBlock:
            cidr: 169.254.169.254/32
---
# Source: jupyterhub/templates/proxy/netpol.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  podSelector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: jupyterhub
  policyTypes:
    - Ingress
    - Egress

  # IMPORTANT:
  # NetworkPolicy's ingress "from" and egress "to" rule specifications require
  # great attention to detail. A quick summary is:
  #
  # 1. You can provide "from"/"to" rules that provide access either ports or a
  #    subset of ports.
  # 2. You can for each "from"/"to" rule provide any number of
  #    "sources"/"destinations" of four different kinds.
  #    - podSelector                        - targets pods with a certain label in the same namespace as the NetworkPolicy
  #    - namespaceSelector                  - targets all pods running in namespaces with a certain label
  #    - namespaceSelector and podSelector  - targets pods with a certain label running in namespaces with a certain label
  #    - ipBlock                            - targets network traffic from/to a set of IP address ranges
  #
  # Read more at: https://kubernetes.io/docs/concepts/services-networking/network-policies/#behavior-of-to-and-from-selectors
  #
  ingress:
    # allow incoming traffic to these ports independent of source
    - ports:
      - port: http
      - port: https

    # allowed pods (hub.jupyter.org/network-access-proxy-http) --> proxy (http/https port)
    - ports:
        - port: http
      from:
        # source 1 - labeled pods
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-proxy-http: "true"

    # allowed pods (hub.jupyter.org/network-access-proxy-api) --> proxy (api port)
    - ports:
        - port: api
      from:
        # source 1 - labeled pods
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-proxy-api: "true"

  egress:
    # proxy --> hub
    - to:
        - podSelector:
            matchLabels:
              component: hub
              app: jupyterhub
              release: jupyterhub
      ports:
        - port: 8081

    # proxy --> singleuser-server
    - to:
        - podSelector:
            matchLabels:
              component: singleuser-server
              app: jupyterhub
              release: jupyterhub
      ports:
        - port: 8888
    
    # Allow outbound connections to the DNS port in the private IP ranges
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
      to:
        - ipBlock:
            cidr: 10.0.0.0/8
        - ipBlock:
            cidr: 172.16.0.0/12
        - ipBlock:
            cidr: 192.168.0.0/16
    # Allow outbound connections to non-private IP ranges
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              # As part of this rule, don't:
              # - allow outbound connections to private IP
              - 10.0.0.0/8
              - 172.16.0.0/12
              - 192.168.0.0/16
              # - allow outbound connections to the cloud metadata server
              - 169.254.169.254/32
    # Allow outbound connections to private IP ranges
    - to:
        - ipBlock:
            cidr: 10.0.0.0/8
        - ipBlock:
            cidr: 172.16.0.0/12
        - ipBlock:
            cidr: 192.168.0.0/16
    # Allow outbound connections to the cloud metadata server
    - to:
        - ipBlock:
            cidr: 169.254.169.254/32
---
# Source: jupyterhub/templates/singleuser/netpol.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: singleuser
  labels:
    component: singleuser
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  podSelector:
    matchLabels:
      component: singleuser-server
      app: jupyterhub
      release: jupyterhub
  policyTypes:
    - Ingress
    - Egress

  # IMPORTANT:
  # NetworkPolicy's ingress "from" and egress "to" rule specifications require
  # great attention to detail. A quick summary is:
  #
  # 1. You can provide "from"/"to" rules that provide access either ports or a
  #    subset of ports.
  # 2. You can for each "from"/"to" rule provide any number of
  #    "sources"/"destinations" of four different kinds.
  #    - podSelector                        - targets pods with a certain label in the same namespace as the NetworkPolicy
  #    - namespaceSelector                  - targets all pods running in namespaces with a certain label
  #    - namespaceSelector and podSelector  - targets pods with a certain label running in namespaces with a certain label
  #    - ipBlock                            - targets network traffic from/to a set of IP address ranges
  #
  # Read more at: https://kubernetes.io/docs/concepts/services-networking/network-policies/#behavior-of-to-and-from-selectors
  #
  ingress:

    # allowed pods (hub.jupyter.org/network-access-singleuser) --> singleuser-server
    - ports:
        - port: notebook-port
      from:
        # source 1 - labeled pods
        - podSelector:
            matchLabels:
              hub.jupyter.org/network-access-singleuser: "true"

  egress:
    # singleuser-server --> hub
    - to:
        - podSelector:
            matchLabels:
              component: hub
              app: jupyterhub
              release: jupyterhub
      ports:
        - port: 8081

    # singleuser-server --> proxy
    # singleuser-server --> autohttps
    #
    # While not critical for core functionality, a user or library code may rely
    # on communicating with the proxy or autohttps pods via a k8s Service it can
    # detected from well known environment variables.
    #
    - to:
        - podSelector:
            matchLabels:
              component: proxy
              app: jupyterhub
              release: jupyterhub
      ports:
        - port: 8000
    - to:
        - podSelector:
            matchLabels:
              component: autohttps
              app: jupyterhub
              release: jupyterhub
      ports:
        - port: 8080
        - port: 8443
    
    # Allow outbound connections to the DNS port in the private IP ranges
    - ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
      to:
        - ipBlock:
            cidr: 10.0.0.0/8
        - ipBlock:
            cidr: 172.16.0.0/12
        - ipBlock:
            cidr: 192.168.0.0/16
    # Allow outbound connections to non-private IP ranges
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              # As part of this rule, don't:
              # - allow outbound connections to private IP
              - 10.0.0.0/8
              - 172.16.0.0/12
              - 192.168.0.0/16
              # - allow outbound connections to the cloud metadata server
              - 169.254.169.254/32
---
# Source: jupyterhub/templates/scheduling/user-placeholder/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: user-placeholder
  labels:
    component: user-placeholder
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  minAvailable: 0
  selector:
    matchLabels:
      component: user-placeholder
      app: jupyterhub
      release: jupyterhub
---
# Source: jupyterhub/templates/scheduling/user-scheduler/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      component: user-scheduler
      app: jupyterhub
      release: jupyterhub
---
# Source: jupyterhub/templates/hub/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
---
# Source: jupyterhub/templates/scheduling/user-scheduler/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
---
# Source: jupyterhub/templates/hub/secret.yaml
kind: Secret
apiVersion: v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
type: Opaque
data:
  values.yaml: "Chart:
  Name: jupyterhub
  Version: 3.0.0-0.dev.git.6171.h0f4b2d0b
Release:
  Name: jupyterhub
  Namespace: jupyterhub
  Service: Helm
cull:
  adminUsers: true
  concurrency: 10
  enabled: true
  every: 600
  maxAge: 0
  removeNamedServers: false
  timeout: 3600
  users: false
custom: {}
debug:
  enabled: false
fullnameOverride: ""
global:
  safeToShowValues: false
hub:
  activeServerLimit: null
  allowNamedServers: false
  annotations: {}
  args: []
  authenticatePrometheus: null
  baseUrl: /
  command: []
  concurrentSpawnLimit: 64
  config:
    JupyterHub:
      admin_access: true
      authenticator_class: dummy
  consecutiveFailureLimit: 5
  containerSecurityContext:
    allowPrivilegeEscalation: false
    runAsGroup: 1000
    runAsUser: 1000
  cookieSecret: null
  db:
    password: null
    pvc:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      selector: {}
      storage: 1Gi
      storageClassName: null
      subPath: null
    type: sqlite-pvc
    upgrade: null
    url: null
  deploymentStrategy:
    type: Recreate
  existingSecret: null
  extraConfig:
    00_bind_url_config: ""
  extraContainers: []
  extraEnv:
    OAUTH_CLIENT_ID:
      valueFrom:
        secretKeyRef:
          key: OAUTH_CLIENT_ID
          name: jupyterhub-oauth-keycloak
    OAUTH_CLIENT_SECRET:
      valueFrom:
        secretKeyRef:
          key: OAUTH_CLIENT_SECRET
          name: jupyterhub-oauth-keycloak
    OAUTH_LOGOUT_REDIRECT_URL:
      valueFrom:
        secretKeyRef:
          key: OAUTH_LOGOUT_REDIRECT_URL
          name: jupyterhub-oauth-keycloak
    OAUTH2_AUTHORIZE_URL:
      valueFrom:
        secretKeyRef:
          key: OAUTH2_AUTHORIZE_URL
          name: jupyterhub-oauth-keycloak
    OAUTH2_TOKEN_URL:
      valueFrom:
        secretKeyRef:
          key: OAUTH2_TOKEN_URL
          name: jupyterhub-oauth-keycloak
    OAUTH2_USERDATA_URL:
      valueFrom:
        secretKeyRef:
          key: OAUTH2_USERDATA_URL
          name: jupyterhub-oauth-keycloak
    PYTHONPATH: /app/custom:$(PYTHONPATH)
  extraFiles:
    bind_url_config:
      mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/00_bind_url_config.py
      stringData: |
        c.JupyterHub.bind_url = 'https://jupyterhub.prod.k8s.86labs.com'
        c.JupyterHub.admin_access = True
    custom_kube_spawner:
      mountPath: /app/custom/custom_kube_spawner.py
      stringData: "import json\nfrom kubespawner import KubeSpawner\nfrom kubernetes_asyncio.client
        import models as k8s\n\nclass CustomKubeSpawner(KubeSpawner):\n    def auth_state_hook(self,
        spawner, auth_state):\n        \"\"\"\n        This user hook attaches the
        user information to the spawner,\n        which we can use when generating
        the profile list.\n        This allows us to have kubernetes overrides base
        on OAuth2 attributes.\n        \"\"\"\n        userinfo = auth_state.get(\"oauth_user\")\n
        \       self.log.info(f\"got user info {userinfo}\")\n        spawner.userinfo
        = userinfo\n\n    def profile_list(self, spawner):\n        \"\"\"\n        The
        profile list for the KubeSpawner can be a callable from the Spawner object\n
        \       Allowing us to dynamically generate profiles based on the user info\n
        \       \"\"\"\n        user_info = spawner.userinfo\n        image_options
        = {\"display_name\": \"Image\", \"choices\": {\"custom\": {\"display_name\":
        \"Custom Single Server\", \"kubespawner_override\": {\"image\": \"gfeldman8/jupyterhub-k8s-singleuser:latest\"}
        } , \"pytorch2\": {\"display_name\": \"Python 3 Training Notebook 2\", \"kubespawner_override\":
        {\"image\": \"training/python:2022.01.01\"} } }}\n        return [{'display_name':
        f'Training Env for {user_info.get(\"name\")}', 'slug': 'training-python',
        'default': True, \"profile_options\": {\"image\": image_options} }, {'display_name':
        f'Prod Env for {user_info.get(\"name\")}','slug': 'prod-python','default':
        False}]\n\n    def get_env(self):\n        env = super().get_env()\n        if
        hasattr(self,'userinfo'):\n            env['PROP_USER_INFO'] = self.userinfo.get(\"name\",
        \"\")\n        if hasattr(self, 'user_options'):\n            env['PROP_USER_PROFILE']
        = self.user_options.get(\"profile\",\"\")\n        self.log.info(f\"Got environment
        variables {env}\")\n        return env\n    \n    def get_service_account_from_user_info(self,
        spawner):\n        user_info = spawner.userinfo\n        self.log.info(f\"got
        user info in modify_pod_hook {user_info}\")\n        return \"jupyterhub-user\"\n
        \   \n    def modify_pod_hook(self, spawner, pod: k8s.V1Pod):\n        # set
        service account based on user info\n        pod.spec.service_account = self.get_service_account_from_user_info(spawner)\n
        \       pod.spec.service_account_name = self.get_service_account_from_user_info(spawner)\n
        \       pod.spec.automount_service_account_token = True\n        # set user
        root to read only file system so they do not store files there permanently\n
        \       pod.spec.volumes.append(k8s.V1Volume(name=\"jupyter-root\",empty_dir=k8s.V1EmptyDirVolumeSource()))\n
        \       pod.spec.containers[0].volume_mounts.append(k8s.V1VolumeMount(name=\"jupyter-root\",
        mount_path=\"/tmp/container\",read_only=False))\n        \n        self.log.info(f\"got
        pod in modify_pod_hook {pod}\")\n        return pod"
    keycloak_authenticator:
      mountPath: /app/custom/keycloak_authenticator.py
      stringData: "from oauthenticator.generic import GenericOAuthenticator\n# subclassing
        the GenericOAuthenticator in case we need to \n# enrich the user info\n# https://github.com/jupyterhub/oauthenticator/blob/main/oauthenticator/generic.py\n\nclass
        KeycloakAuthenticator(GenericOAuthenticator):\n    login_service = 'keycloak'\n
        \   userdata_params = {\"state\": \"state\"}\n    username_key =  \"preferred_username\"\n
        \   admin_groups = [\"jupyterhub-admin\"]\n    allowed_groups = [\"jupyterhub-user\",
        \"jupyterhub-admin\"]\n    enable_auth_state =  True\n\n    def claim_groups_key(self,
        userinfo_data):\n        return userinfo_data['jupyterhub_groups']\n\n    \n"
    keycloak_config:
      mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/01_keycloak_config.py
      stringData: |
        from keycloak_authenticator import KeycloakAuthenticator
        c.JupyterHub.authenticator_class = KeycloakAuthenticator
    spawner_config:
      mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/02_spawner_config.py
      stringData: |
        from custom_kube_spawner import CustomKubeSpawner
        c.JupyterHub.spawner_class = CustomKubeSpawner
  extraPodSpec: {}
  extraVolumeMounts: []
  extraVolumes: []
  image:
    name: jupyterhub/k8s-hub
    pullPolicy: null
    pullSecrets: []
    tag: 3.0.0-0.dev.git.6167.h43af1c55
  initContainers: []
  labels: {}
  lifecycle: {}
  livenessProbe:
    enabled: true
    failureThreshold: 30
    initialDelaySeconds: 300
    periodSeconds: 10
    timeoutSeconds: 3
  loadRoles: {}
  namedServerLimitPerUser: null
  networkPolicy:
    allowedIngressPorts: []
    egress: []
    egressAllowRules:
      cloudMetadataServer: true
      dnsPortsPrivateIPs: true
      nonPrivateIPs: true
      privateIPs: true
    enabled: true
    ingress: []
    interNamespaceAccessLabels: ignore
  nodeSelector: {}
  pdb:
    enabled: false
    maxUnavailable: null
    minAvailable: 1
  podSecurityContext:
    fsGroup: 1000
  readinessProbe:
    enabled: true
    failureThreshold: 1000
    initialDelaySeconds: 0
    periodSeconds: 2
    timeoutSeconds: 1
  redirectToServer: null
  resources: {}
  revisionHistoryLimit: null
  service:
    annotations: {}
    extraPorts: []
    loadBalancerIP: null
    ports:
      nodePort: null
    type: ClusterIP
  serviceAccount:
    annotations: {}
    create: true
    name: null
  services: {}
  shutdownOnLogout: null
  templatePaths: []
  templateVars: {}
  tolerations: []
imagePullSecret:
  automaticReferenceInjection: true
  create: false
  email: null
  password: null
  registry: null
  username: null
imagePullSecrets: []
ingress:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  enabled: true
  hosts:
  - jupyterhub.prod.k8s.86labs.com
  ingressClassName: nginx
  pathSuffix: null
  pathType: Prefix
  tls:
  - hosts:
    - jupyterhub.prod.k8s.86labs.com
    secretName: jupyterhub-prod-k8s-86labs-com
prePuller:
  annotations: {}
  containerSecurityContext:
    allowPrivilegeEscalation: false
    runAsGroup: 65534
    runAsUser: 65534
  continuous:
    enabled: true
  extraImages: {}
  extraTolerations: []
  hook:
    containerSecurityContext:
      allowPrivilegeEscalation: false
      runAsGroup: 65534
      runAsUser: 65534
    enabled: true
    image:
      name: jupyterhub/k8s-image-awaiter
      pullPolicy: null
      pullSecrets: []
      tag: 3.0.0-0.dev.git.6114.hc0d0b3b0
    nodeSelector: {}
    podSchedulingWaitDuration: 10
    pullOnlyOnChanges: true
    resources: {}
    serviceAccount:
      annotations: {}
      create: true
      name: null
    tolerations: []
  labels: {}
  pause:
    containerSecurityContext:
      allowPrivilegeEscalation: false
      runAsGroup: 65534
      runAsUser: 65534
    image:
      name: registry.k8s.io/pause
      pullPolicy: null
      pullSecrets: []
      tag: "3.9"
  pullProfileListImages: true
  resources: {}
  revisionHistoryLimit: null
proxy:
  annotations: {}
  chp:
    containerSecurityContext:
      allowPrivilegeEscalation: false
      runAsGroup: 65534
      runAsUser: 65534
    defaultTarget: null
    errorTarget: null
    extraCommandLineFlags: []
    extraEnv: {}
    extraPodSpec: {}
    image:
      name: jupyterhub/configurable-http-proxy
      pullPolicy: null
      pullSecrets: []
      tag: 4.5.5
    livenessProbe:
      enabled: true
      failureThreshold: 30
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 3
    networkPolicy:
      allowedIngressPorts:
      - http
      - https
      egress: []
      egressAllowRules:
        cloudMetadataServer: true
        dnsPortsPrivateIPs: true
        nonPrivateIPs: true
        privateIPs: true
      enabled: true
      ingress: []
      interNamespaceAccessLabels: ignore
    nodeSelector: {}
    pdb:
      enabled: false
      maxUnavailable: null
      minAvailable: 1
    readinessProbe:
      enabled: true
      failureThreshold: 1000
      initialDelaySeconds: 0
      periodSeconds: 2
      timeoutSeconds: 1
    resources: {}
    revisionHistoryLimit: null
    tolerations: []
  deploymentStrategy:
    rollingUpdate: null
    type: Recreate
  https:
    enabled: true
    hosts: []
    letsencrypt:
      acmeServer: https://acme-v02.api.letsencrypt.org/directory
      contactEmail: null
    manual:
      cert: null
      key: null
    secret:
      crt: tls.crt
      key: tls.key
      name: null
    type: offload
  labels: {}
  secretSync:
    containerSecurityContext:
      allowPrivilegeEscalation: false
      runAsGroup: 65534
      runAsUser: 65534
    image:
      name: jupyterhub/k8s-secret-sync
      pullPolicy: null
      pullSecrets: []
      tag: 3.0.0-0.dev.git.6171.h0f4b2d0b
    resources: {}
  secretToken: null
  service:
    annotations: {}
    disableHttpPort: false
    extraPorts: []
    labels: {}
    loadBalancerIP: null
    loadBalancerSourceRanges: []
    nodePorts:
      http: null
      https: null
    type: ClusterIP
  traefik:
    containerSecurityContext:
      allowPrivilegeEscalation: false
      runAsGroup: 65534
      runAsUser: 65534
    extraDynamicConfig: {}
    extraEnv: {}
    extraInitContainers: []
    extraPodSpec: {}
    extraPorts: []
    extraStaticConfig: {}
    extraVolumeMounts: []
    extraVolumes: []
    hsts:
      includeSubdomains: false
      maxAge: 15724800
      preload: false
    image:
      name: traefik
      pullPolicy: null
      pullSecrets: []
      tag: v2.10.1
    labels: {}
    networkPolicy:
      allowedIngressPorts:
      - http
      - https
      egress: []
      egressAllowRules:
        cloudMetadataServer: true
        dnsPortsPrivateIPs: true
        nonPrivateIPs: true
        privateIPs: true
      enabled: true
      ingress: []
      interNamespaceAccessLabels: ignore
    nodeSelector: {}
    pdb:
      enabled: false
      maxUnavailable: null
      minAvailable: 1
    resources: {}
    revisionHistoryLimit: null
    serviceAccount:
      annotations: {}
      create: true
      name: null
    tolerations: []
rbac:
  create: true
scheduling:
  corePods:
    nodeAffinity:
      matchNodePurpose: prefer
    tolerations:
    - effect: NoSchedule
      key: hub.jupyter.org/dedicated
      operator: Equal
      value: core
    - effect: NoSchedule
      key: hub.jupyter.org_dedicated
      operator: Equal
      value: core
  podPriority:
    defaultPriority: 0
    enabled: false
    globalDefault: false
    imagePullerPriority: -5
    userPlaceholderPriority: -10
  userPlaceholder:
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      runAsGroup: 65534
      runAsUser: 65534
    enabled: true
    image:
      name: registry.k8s.io/pause
      pullPolicy: null
      pullSecrets: []
      tag: "3.9"
    labels: {}
    replicas: 0
    resources: {}
    revisionHistoryLimit: null
  userPods:
    nodeAffinity:
      matchNodePurpose: prefer
    tolerations:
    - effect: NoSchedule
      key: hub.jupyter.org/dedicated
      operator: Equal
      value: user
    - effect: NoSchedule
      key: hub.jupyter.org_dedicated
      operator: Equal
      value: user
  userScheduler:
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      runAsGroup: 65534
      runAsUser: 65534
    enabled: true
    extraPodSpec: {}
    image:
      name: registry.k8s.io/kube-scheduler
      pullPolicy: null
      pullSecrets: []
      tag: v1.26.5
    labels: {}
    logLevel: 4
    nodeSelector: {}
    pdb:
      enabled: true
      maxUnavailable: 1
      minAvailable: null
    pluginConfig:
    - args:
        scoringStrategy:
          resources:
          - name: cpu
            weight: 1
          - name: memory
            weight: 1
          type: MostAllocated
      name: NodeResourcesFit
    plugins:
      score:
        disabled:
        - name: NodeResourcesBalancedAllocation
        - name: NodeAffinity
        - name: InterPodAffinity
        - name: NodeResourcesFit
        - name: ImageLocality
        enabled:
        - name: NodeAffinity
          weight: 14631
        - name: InterPodAffinity
          weight: 1331
        - name: NodeResourcesFit
          weight: 121
        - name: ImageLocality
          weight: 11
    replicas: 1
    resources: {}
    revisionHistoryLimit: null
    serviceAccount:
      annotations: {}
      create: true
      name: null
    tolerations: []
singleuser:
  allowPrivilegeEscalation: false
  cloudMetadata:
    blockWithIptables: false
    ip: 169.254.169.254
  cmd: jupyterhub-singleuser
  cpu:
    guarantee: null
    limit: null
  defaultUrl: null
  events: true
  extraAnnotations: {}
  extraContainers: []
  extraEnv:
    JUPYTERHUB_SINGLEUSER_APP: jupyter_server.serverapp.ServerApp
  extraFiles:
    notebook_config:
      mountPath: /usr/local/etc/jupyter/jupyter_server_config.py
      stringData: |
        import os
        from hybridcontents import HybridContentsManager
        from s3contents.gcs import GCSContentsManager
        from notebook.services.contents.largefilemanager import LargeFileManager

        os.environ['TEST_USER_PROPAGATION'] = os.environ.get('PROP_USER_INFO')
        os.environ['TEST_PROFILE_PROPAGATION'] = os.environ.get('PROP_USER_PROFILE')

        c.ServerApp.contents_manager_class = HybridContentsManager
        c.HybridContentsManager.manager_classes = {
            "": GCSContentsManager,
            "scratch": LargeFileManager
        }
        c.HybridContentsManager.manager_kwargs = {
          "": {"project": "labs-governance", "bucket": "jupyterhub-86labs-k8s-storage"},
          "scratch": {"root_dir": "/home/jovyan"}
        }
        c.HybridContentsManager.managers = {
          k: v(**c.HybridContentsManager.manager_kwargs.get(k)) for k,v in c.HybridContentsManager.manager_classes.items()
        }
  extraLabels:
    hub.jupyter.org/network-access-hub: "true"
  extraNodeAffinity:
    preferred: []
    required: []
  extraPodAffinity:
    preferred: []
    required: []
  extraPodAntiAffinity:
    preferred: []
    required: []
  extraPodConfig: {}
  extraResource:
    guarantees: {}
    limits: {}
  extraTolerations:
  - effect: NoSchedule
    key: ephemeral
    operator: Equal
    value: "true"
  fsGid: 100
  image:
    name: jupyterhub/k8s-singleuser-sample
    pullPolicy: null
    pullSecrets: []
    tag: 3.0.0-0.dev.git.6167.ha69d8ec6
  initContainers: []
  lifecycleHooks: {}
  memory:
    guarantee: 1G
    limit: null
  networkPolicy:
    allowedIngressPorts: []
    egress: []
    egressAllowRules:
      cloudMetadataServer: false
      dnsPortsPrivateIPs: true
      nonPrivateIPs: true
      privateIPs: false
    enabled: true
    ingress: []
    interNamespaceAccessLabels: ignore
  networkTools:
    image:
      name: jupyterhub/k8s-network-tools
      pullPolicy: null
      pullSecrets: []
      tag: 3.0.0-0.dev.git.6159.h50a08fad
    resources: {}
  nodeSelector:
    cloud.google.com/gke-nodepool: ephemeral-pool
  podNameTemplate: null
  profileList: []
  serviceAccountName: null
  startTimeout: 300
  storage:
    capacity: 10Gi
    dynamic:
      pvcNameTemplate: claim-{username}{servername}
      storageAccessModes:
      - ReadWriteOnce
      storageClass: null
      volumeNameTemplate: volume-{username}{servername}
    extraLabels: {}
    extraVolumeMounts: []
    extraVolumes: []
    homeMountPath: /home/jovyan
    static:
      pvcName: null
      subPath: '{username}'
    type: dynamic
  uid: 1000"

  # Any JupyterHub Services api_tokens are exposed in this k8s Secret as a
  # convinience for external services running in the k8s cluster that could
  # mount them directly from this k8s Secret.

  # During Helm template rendering, these values that can be autogenerated for
  # users are set using the following logic:
  #
  # 1. Use chart configuration's value
  # 2. Use k8s Secret's value
  # 3. Use a new autogenerated value
  #
  # hub.config.ConfigurableHTTPProxy.auth_token: for hub to proxy-api authorization (JupyterHub.proxy_auth_token is deprecated)
  # hub.config.JupyterHub.cookie_secret:         for cookie encryption
  # hub.config.CryptKeeper.keys:                 for auth state encryption
  #
  hub.config.ConfigurableHTTPProxy.auth_token: "RTFBYlZ5Tjk5N0NYc0E2U0hVc0EzNFdWSTc4MDczUGxGd1RJcGMwUEF6elI0R2xweWpjTFFnVnZiaXdSUzN4YQ=="
  hub.config.JupyterHub.cookie_secret: "NDM5MDcyNGEzNWYzNWM1ZjRlZGUxNDkwNjI4NjU4NWJkNGI5YjU2MTFjZWVkYzIzNDFlOWM3NDRjNWI5ZmIyZA=="
  hub.config.CryptKeeper.keys: "ZDNjZGE0YmQzMWI4ZDhkY2ViMjY2ZDcwYjFmMzcwZDgwNTQ4ZmRiOTJlM2JkZDY4ODRkZTE1ZGI3OWQ0ZjUwYQ=="
stringData:
  "bind_url_config": |
    c.JupyterHub.bind_url = 'https://jupyterhub.prod.k8s.86labs.com'
    c.JupyterHub.admin_access = True
  "custom_kube_spawner": |
    import json
    from kubespawner import KubeSpawner
    from kubernetes_asyncio.client import models as k8s
    
    class CustomKubeSpawner(KubeSpawner):
        def auth_state_hook(self, spawner, auth_state):
            """
            This user hook attaches the user information to the spawner,
            which we can use when generating the profile list.
            This allows us to have kubernetes overrides base on OAuth2 attributes.
            """
            userinfo = auth_state.get("oauth_user")
            self.log.info(f"got user info {userinfo}")
            spawner.userinfo = userinfo
    
        def profile_list(self, spawner):
            """
            The profile list for the KubeSpawner can be a callable from the Spawner object
            Allowing us to dynamically generate profiles based on the user info
            """
            user_info = spawner.userinfo
            image_options = {"display_name": "Image", "choices": {"custom": {"display_name": "Custom Single Server", "kubespawner_override": {"image": "gfeldman8/jupyterhub-k8s-singleuser:latest"} } , "pytorch2": {"display_name": "Python 3 Training Notebook 2", "kubespawner_override": {"image": "training/python:2022.01.01"} } }}
            return [{'display_name': f'Training Env for {user_info.get("name")}', 'slug': 'training-python', 'default': True, "profile_options": {"image": image_options} }, {'display_name': f'Prod Env for {user_info.get("name")}','slug': 'prod-python','default': False}]
    
        def get_env(self):
            env = super().get_env()
            if hasattr(self,'userinfo'):
                env['PROP_USER_INFO'] = self.userinfo.get("name", "")
            if hasattr(self, 'user_options'):
                env['PROP_USER_PROFILE'] = self.user_options.get("profile","")
            self.log.info(f"Got environment variables {env}")
            return env
        
        def get_service_account_from_user_info(self, spawner):
            user_info = spawner.userinfo
            self.log.info(f"got user info in modify_pod_hook {user_info}")
            return "jupyterhub-user"
        
        def modify_pod_hook(self, spawner, pod: k8s.V1Pod):
            # set service account based on user info
            pod.spec.service_account = self.get_service_account_from_user_info(spawner)
            pod.spec.service_account_name = self.get_service_account_from_user_info(spawner)
            pod.spec.automount_service_account_token = True
            # set user root to read only file system so they do not store files there permanently
            pod.spec.volumes.append(k8s.V1Volume(name="jupyter-root",empty_dir=k8s.V1EmptyDirVolumeSource()))
            pod.spec.containers[0].volume_mounts.append(k8s.V1VolumeMount(name="jupyter-root", mount_path="/tmp/container",read_only=False))
            
            self.log.info(f"got pod in modify_pod_hook {pod}")
            return pod
  "keycloak_authenticator": |
    from oauthenticator.generic import GenericOAuthenticator
    # subclassing the GenericOAuthenticator in case we need to 
    # enrich the user info
    # https://github.com/jupyterhub/oauthenticator/blob/main/oauthenticator/generic.py
    
    class KeycloakAuthenticator(GenericOAuthenticator):
        login_service = 'keycloak'
        userdata_params = {"state": "state"}
        username_key =  "preferred_username"
        admin_groups = ["jupyterhub-admin"]
        allowed_groups = ["jupyterhub-user", "jupyterhub-admin"]
        enable_auth_state =  True
    
        def claim_groups_key(self, userinfo_data):
            return userinfo_data['jupyterhub_groups']
    
        
  "keycloak_config": |
    from keycloak_authenticator import KeycloakAuthenticator
    c.JupyterHub.authenticator_class = KeycloakAuthenticator
  "spawner_config": |
    from custom_kube_spawner import CustomKubeSpawner
    c.JupyterHub.spawner_class = CustomKubeSpawner
---
# Source: jupyterhub/templates/singleuser/secret.yaml
kind: Secret
apiVersion: v1
metadata:
  name: singleuser
  labels:
    component: singleuser
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
type: Opaque
stringData:
  "notebook_config": |
    import os
    from hybridcontents import HybridContentsManager
    from s3contents.gcs import GCSContentsManager
    from notebook.services.contents.largefilemanager import LargeFileManager
    
    os.environ['TEST_USER_PROPAGATION'] = os.environ.get('PROP_USER_INFO')
    os.environ['TEST_PROFILE_PROPAGATION'] = os.environ.get('PROP_USER_PROFILE')
    
    c.ServerApp.contents_manager_class = HybridContentsManager
    c.HybridContentsManager.manager_classes = {
        "": GCSContentsManager,
        "scratch": LargeFileManager
    }
    c.HybridContentsManager.manager_kwargs = {
      "": {"project": "labs-governance", "bucket": "jupyterhub-86labs-k8s-storage"},
      "scratch": {"root_dir": "/home/jovyan"}
    }
    c.HybridContentsManager.managers = {
      k: v(**c.HybridContentsManager.manager_kwargs.get(k)) for k,v in c.HybridContentsManager.manager_classes.items()
    }
---
# Source: jupyterhub/templates/hub/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
data:
  fullname: ""
  fullname-dash: ""
  hub: "hub"
  hub-serviceaccount: "hub"
  hub-existing-secret: ""
  hub-existing-secret-or-default: "hub"
  hub-pvc: "hub-db-dir"
  proxy: "proxy"
  proxy-api: "proxy-api"
  proxy-http: "proxy-http"
  proxy-public: "proxy-public"
  proxy-public-tls: "proxy-public-tls-acme"
  proxy-public-manual-tls: "proxy-public-manual-tls"
  autohttps: "autohttps"
  autohttps-serviceaccount: "autohttps"
  user-scheduler-deploy: "user-scheduler"
  user-scheduler-serviceaccount: "user-scheduler"
  user-scheduler-lock: "user-scheduler-lock"
  user-placeholder: "user-placeholder"
  image-puller-priority: "jupyterhub-image-puller-priority"
  hook-image-awaiter: "hook-image-awaiter"
  hook-image-awaiter-serviceaccount: "hook-image-awaiter"
  hook-image-puller: "hook-image-puller"
  continuous-image-puller: "continuous-image-puller"
  singleuser: "singleuser"
  image-pull-secret: "image-pull-secret"
  ingress: "jupyterhub"
  priority: "jupyterhub-default-priority"
  user-placeholder-priority: "jupyterhub-user-placeholder-priority"
  user-scheduler: "jupyterhub-user-scheduler"
  jupyterhub_config.py: |
    # load the config object (satisfies linters)
    c = get_config()  # noqa
  
    import glob
    import os
    import re
    import sys
  
    from jupyterhub.utils import url_path_join
    from kubernetes_asyncio import client
    from tornado.httpclient import AsyncHTTPClient
  
    # Make sure that modules placed in the same directory as the jupyterhub config are added to the pythonpath
    configuration_directory = os.path.dirname(os.path.realpath(__file__))
    sys.path.insert(0, configuration_directory)
  
    from z2jh import (
        get_config,
        get_name,
        get_name_env,
        get_secret_value,
        set_config_if_not_none,
    )
  
  
    def camelCaseify(s):
        """convert snake_case to camelCase
  
        For the common case where some_value is set from someValue
        so we don't have to specify the name twice.
        """
        return re.sub(r"_([a-z])", lambda m: m.group(1).upper(), s)
  
  
    # Configure JupyterHub to use the curl backend for making HTTP requests,
    # rather than the pure-python implementations. The default one starts
    # being too slow to make a large number of requests to the proxy API
    # at the rate required.
    AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
  
    c.JupyterHub.spawner_class = "kubespawner.KubeSpawner"
  
    # Connect to a proxy running in a different pod. Note that *_SERVICE_*
    # environment variables are set by Kubernetes for Services
    c.ConfigurableHTTPProxy.api_url = (
        f'http://{get_name("proxy-api")}:{get_name_env("proxy-api", "_SERVICE_PORT")}'
    )
    c.ConfigurableHTTPProxy.should_start = False
  
    # Do not shut down user pods when hub is restarted
    c.JupyterHub.cleanup_servers = False
  
    # Check that the proxy has routes appropriately setup
    c.JupyterHub.last_activity_interval = 60
  
    # Don't wait at all before redirecting a spawning user to the progress page
    c.JupyterHub.tornado_settings = {
        "slow_spawn_timeout": 0,
    }
  
  
    # configure the hub db connection
    db_type = get_config("hub.db.type")
    if db_type == "sqlite-pvc":
        c.JupyterHub.db_url = "sqlite:///jupyterhub.sqlite"
    elif db_type == "sqlite-memory":
        c.JupyterHub.db_url = "sqlite://"
    else:
        set_config_if_not_none(c.JupyterHub, "db_url", "hub.db.url")
    db_password = get_secret_value("hub.db.password", None)
    if db_password is not None:
        if db_type == "mysql":
            os.environ["MYSQL_PWD"] = db_password
        elif db_type == "postgres":
            os.environ["PGPASSWORD"] = db_password
        else:
            print(f"Warning: hub.db.password is ignored for hub.db.type={db_type}")
  
  
    # c.JupyterHub configuration from Helm chart's configmap
    for trait, cfg_key in (
        ("concurrent_spawn_limit", None),
        ("active_server_limit", None),
        ("base_url", None),
        ("allow_named_servers", None),
        ("named_server_limit_per_user", None),
        ("authenticate_prometheus", None),
        ("redirect_to_server", None),
        ("shutdown_on_logout", None),
        ("template_paths", None),
        ("template_vars", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.JupyterHub, trait, "hub." + cfg_key)
  
    # hub_bind_url configures what the JupyterHub process within the hub pod's
    # container should listen to.
    hub_container_port = 8081
    c.JupyterHub.hub_bind_url = f"http://:{hub_container_port}"
  
    # hub_connect_url is the URL for connecting to the hub for use by external
    # JupyterHub services such as the proxy. Note that *_SERVICE_* environment
    # variables are set by Kubernetes for Services.
    c.JupyterHub.hub_connect_url = (
        f'http://{get_name("hub")}:{get_name_env("hub", "_SERVICE_PORT")}'
    )
  
    # implement common labels
    # this duplicates the jupyterhub.commonLabels helper
    common_labels = c.KubeSpawner.common_labels = {}
    common_labels["app"] = get_config(
        "nameOverride",
        default=get_config("Chart.Name", "jupyterhub"),
    )
    common_labels["heritage"] = "jupyterhub"
    chart_name = get_config("Chart.Name")
    chart_version = get_config("Chart.Version")
    if chart_name and chart_version:
        common_labels["chart"] = "{}-{}".format(
            chart_name,
            chart_version.replace("+", "_"),
        )
    release = get_config("Release.Name")
    if release:
        common_labels["release"] = release
  
    c.KubeSpawner.namespace = os.environ.get("POD_NAMESPACE", "default")
  
    # Max number of consecutive failures before the Hub restarts itself
    # requires jupyterhub 0.9.2
    set_config_if_not_none(
        c.Spawner,
        "consecutive_failure_limit",
        "hub.consecutiveFailureLimit",
    )
  
    for trait, cfg_key in (
        ("pod_name_template", None),
        ("start_timeout", None),
        ("image_pull_policy", "image.pullPolicy"),
        # ('image_pull_secrets', 'image.pullSecrets'), # Managed manually below
        ("events_enabled", "events"),
        ("extra_labels", None),
        ("extra_annotations", None),
        # ("allow_privilege_escalation", None), # Managed manually below
        ("uid", None),
        ("fs_gid", None),
        ("service_account", "serviceAccountName"),
        ("storage_extra_labels", "storage.extraLabels"),
        # ("tolerations", "extraTolerations"), # Managed manually below
        ("node_selector", None),
        ("node_affinity_required", "extraNodeAffinity.required"),
        ("node_affinity_preferred", "extraNodeAffinity.preferred"),
        ("pod_affinity_required", "extraPodAffinity.required"),
        ("pod_affinity_preferred", "extraPodAffinity.preferred"),
        ("pod_anti_affinity_required", "extraPodAntiAffinity.required"),
        ("pod_anti_affinity_preferred", "extraPodAntiAffinity.preferred"),
        ("lifecycle_hooks", None),
        ("init_containers", None),
        ("extra_containers", None),
        ("mem_limit", "memory.limit"),
        ("mem_guarantee", "memory.guarantee"),
        ("cpu_limit", "cpu.limit"),
        ("cpu_guarantee", "cpu.guarantee"),
        ("extra_resource_limits", "extraResource.limits"),
        ("extra_resource_guarantees", "extraResource.guarantees"),
        ("environment", "extraEnv"),
        ("profile_list", None),
        ("extra_pod_config", None),
    ):
        if cfg_key is None:
            cfg_key = camelCaseify(trait)
        set_config_if_not_none(c.KubeSpawner, trait, "singleuser." + cfg_key)
  
    image = get_config("singleuser.image.name")
    if image:
        tag = get_config("singleuser.image.tag")
        if tag:
            image = f"{image}:{tag}"
  
        c.KubeSpawner.image = image
  
    # allow_privilege_escalation defaults to False in KubeSpawner 2+. Since its a
    # property where None, False, and True all are valid values that users of the
    # Helm chart may want to set, we can't use the set_config_if_not_none helper
    # function as someone may want to override the default False value to None.
    #
    c.KubeSpawner.allow_privilege_escalation = get_config(
        "singleuser.allowPrivilegeEscalation"
    )
  
    # Combine imagePullSecret.create (single), imagePullSecrets (list), and
    # singleuser.image.pullSecrets (list).
    image_pull_secrets = []
    if get_config("imagePullSecret.automaticReferenceInjection") and get_config(
        "imagePullSecret.create"
    ):
        image_pull_secrets.append(get_name("image-pull-secret"))
    if get_config("imagePullSecrets"):
        image_pull_secrets.extend(get_config("imagePullSecrets"))
    if get_config("singleuser.image.pullSecrets"):
        image_pull_secrets.extend(get_config("singleuser.image.pullSecrets"))
    if image_pull_secrets:
        c.KubeSpawner.image_pull_secrets = image_pull_secrets
  
    # scheduling:
    if get_config("scheduling.userScheduler.enabled"):
        c.KubeSpawner.scheduler_name = get_name("user-scheduler")
    if get_config("scheduling.podPriority.enabled"):
        c.KubeSpawner.priority_class_name = get_name("priority")
  
    # add node-purpose affinity
    match_node_purpose = get_config("scheduling.userPods.nodeAffinity.matchNodePurpose")
    if match_node_purpose:
        node_selector = dict(
            matchExpressions=[
                dict(
                    key="hub.jupyter.org/node-purpose",
                    operator="In",
                    values=["user"],
                )
            ],
        )
        if match_node_purpose == "prefer":
            c.KubeSpawner.node_affinity_preferred.append(
                dict(
                    weight=100,
                    preference=node_selector,
                ),
            )
        elif match_node_purpose == "require":
            c.KubeSpawner.node_affinity_required.append(node_selector)
        elif match_node_purpose == "ignore":
            pass
        else:
            raise ValueError(
                f"Unrecognized value for matchNodePurpose: {match_node_purpose}"
            )
  
    # Combine the common tolerations for user pods with singleuser tolerations
    scheduling_user_pods_tolerations = get_config("scheduling.userPods.tolerations", [])
    singleuser_extra_tolerations = get_config("singleuser.extraTolerations", [])
    tolerations = scheduling_user_pods_tolerations + singleuser_extra_tolerations
    if tolerations:
        c.KubeSpawner.tolerations = tolerations
  
    # Configure dynamically provisioning pvc
    storage_type = get_config("singleuser.storage.type")
    if storage_type == "dynamic":
        pvc_name_template = get_config("singleuser.storage.dynamic.pvcNameTemplate")
        c.KubeSpawner.pvc_name_template = pvc_name_template
        volume_name_template = get_config("singleuser.storage.dynamic.volumeNameTemplate")
        c.KubeSpawner.storage_pvc_ensure = True
        set_config_if_not_none(
            c.KubeSpawner, "storage_class", "singleuser.storage.dynamic.storageClass"
        )
        set_config_if_not_none(
            c.KubeSpawner,
            "storage_access_modes",
            "singleuser.storage.dynamic.storageAccessModes",
        )
        set_config_if_not_none(
            c.KubeSpawner, "storage_capacity", "singleuser.storage.capacity"
        )
  
        # Add volumes to singleuser pods
        c.KubeSpawner.volumes = [
            {
                "name": volume_name_template,
                "persistentVolumeClaim": {"claimName": pvc_name_template},
            }
        ]
        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": volume_name_template,
            }
        ]
    elif storage_type == "static":
        pvc_claim_name = get_config("singleuser.storage.static.pvcName")
        c.KubeSpawner.volumes = [
            {"name": "home", "persistentVolumeClaim": {"claimName": pvc_claim_name}}
        ]
  
        c.KubeSpawner.volume_mounts = [
            {
                "mountPath": get_config("singleuser.storage.homeMountPath"),
                "name": "home",
                "subPath": get_config("singleuser.storage.static.subPath"),
            }
        ]
  
    # Inject singleuser.extraFiles as volumes and volumeMounts with data loaded from
    # the dedicated k8s Secret prepared to hold the extraFiles actual content.
    extra_files = get_config("singleuser.extraFiles", {})
    if extra_files:
        volume = {
            "name": "files",
        }
        items = []
        for file_key, file_details in extra_files.items():
            # Each item is a mapping of a key in the k8s Secret to a path in this
            # abstract volume, the goal is to enable us to set the mode /
            # permissions only though so we don't change the mapping.
            item = {
                "key": file_key,
                "path": file_key,
            }
            if "mode" in file_details:
                item["mode"] = file_details["mode"]
            items.append(item)
        volume["secret"] = {
            "secretName": get_name("singleuser"),
            "items": items,
        }
        c.KubeSpawner.volumes.append(volume)
  
        volume_mounts = []
        for file_key, file_details in extra_files.items():
            volume_mounts.append(
                {
                    "mountPath": file_details["mountPath"],
                    "subPath": file_key,
                    "name": "files",
                }
            )
        c.KubeSpawner.volume_mounts.extend(volume_mounts)
  
    # Inject extraVolumes / extraVolumeMounts
    c.KubeSpawner.volumes.extend(get_config("singleuser.storage.extraVolumes", []))
    c.KubeSpawner.volume_mounts.extend(
        get_config("singleuser.storage.extraVolumeMounts", [])
    )
  
    c.JupyterHub.services = []
    c.JupyterHub.load_roles = []
  
    # jupyterhub-idle-culler's permissions are scoped to what it needs only, see
    # https://github.com/jupyterhub/jupyterhub-idle-culler#permissions.
    #
    if get_config("cull.enabled", False):
        jupyterhub_idle_culler_role = {
            "name": "jupyterhub-idle-culler",
            "scopes": [
                "list:users",
                "read:users:activity",
                "read:servers",
                "delete:servers",
                # "admin:users", # dynamically added if --cull-users is passed
            ],
            # assign the role to a jupyterhub service, so it gains these permissions
            "services": ["jupyterhub-idle-culler"],
        }
  
        cull_cmd = ["python3", "-m", "jupyterhub_idle_culler"]
        base_url = c.JupyterHub.get("base_url", "/")
        cull_cmd.append("--url=http://localhost:8081" + url_path_join(base_url, "hub/api"))
  
        cull_timeout = get_config("cull.timeout")
        if cull_timeout:
            cull_cmd.append(f"--timeout={cull_timeout}")
  
        cull_every = get_config("cull.every")
        if cull_every:
            cull_cmd.append(f"--cull-every={cull_every}")
  
        cull_concurrency = get_config("cull.concurrency")
        if cull_concurrency:
            cull_cmd.append(f"--concurrency={cull_concurrency}")
  
        if get_config("cull.users"):
            cull_cmd.append("--cull-users")
            jupyterhub_idle_culler_role["scopes"].append("admin:users")
  
        if not get_config("cull.adminUsers"):
            cull_cmd.append("--cull-admin-users=false")
  
        if get_config("cull.removeNamedServers"):
            cull_cmd.append("--remove-named-servers")
  
        cull_max_age = get_config("cull.maxAge")
        if cull_max_age:
            cull_cmd.append(f"--max-age={cull_max_age}")
  
        c.JupyterHub.services.append(
            {
                "name": "jupyterhub-idle-culler",
                "command": cull_cmd,
            }
        )
        c.JupyterHub.load_roles.append(jupyterhub_idle_culler_role)
  
    for key, service in get_config("hub.services", {}).items():
        # c.JupyterHub.services is a list of dicts, but
        # hub.services is a dict of dicts to make the config mergable
        service.setdefault("name", key)
  
        # As the api_token could be exposed in hub.existingSecret, we need to read
        # it it from there or fall back to the chart managed k8s Secret's value.
        service.pop("apiToken", None)
        service["api_token"] = get_secret_value(f"hub.services.{key}.apiToken")
  
        c.JupyterHub.services.append(service)
  
    for key, role in get_config("hub.loadRoles", {}).items():
        # c.JupyterHub.load_roles is a list of dicts, but
        # hub.loadRoles is a dict of dicts to make the config mergable
        role.setdefault("name", key)
  
        c.JupyterHub.load_roles.append(role)
  
    # respect explicit null command (distinct from unspecified)
    # this avoids relying on KubeSpawner.cmd's default being None
    _unspecified = object()
    specified_cmd = get_config("singleuser.cmd", _unspecified)
    if specified_cmd is not _unspecified:
        c.Spawner.cmd = specified_cmd
  
    set_config_if_not_none(c.Spawner, "default_url", "singleuser.defaultUrl")
  
    cloud_metadata = get_config("singleuser.cloudMetadata", {})
  
    if cloud_metadata.get("blockWithIptables") == True:
        # Use iptables to block access to cloud metadata by default
        network_tools_image_name = get_config("singleuser.networkTools.image.name")
        network_tools_image_tag = get_config("singleuser.networkTools.image.tag")
        network_tools_resources = get_config("singleuser.networkTools.resources")
        ip_block_container = client.V1Container(
            name="block-cloud-metadata",
            image=f"{network_tools_image_name}:{network_tools_image_tag}",
            command=[
                "iptables",
                "-A",
                "OUTPUT",
                "-d",
                cloud_metadata.get("ip", "169.254.169.254"),
                "-j",
                "DROP",
            ],
            security_context=client.V1SecurityContext(
                privileged=True,
                run_as_user=0,
                capabilities=client.V1Capabilities(add=["NET_ADMIN"]),
            ),
            resources=network_tools_resources,
        )
  
        c.KubeSpawner.init_containers.append(ip_block_container)
  
  
    if get_config("debug.enabled", False):
        c.JupyterHub.log_level = "DEBUG"
        c.Spawner.debug = True
  
    # load potentially seeded secrets
    #
    # NOTE: ConfigurableHTTPProxy.auth_token is set through an environment variable
    #       that is set using the chart managed secret.
    c.JupyterHub.cookie_secret = get_secret_value("hub.config.JupyterHub.cookie_secret")
    # NOTE: CryptKeeper.keys should be a list of strings, but we have encoded as a
    #       single string joined with ; in the k8s Secret.
    #
    c.CryptKeeper.keys = get_secret_value("hub.config.CryptKeeper.keys").split(";")
  
    # load hub.config values, except potentially seeded secrets already loaded
    for app, cfg in get_config("hub.config", {}).items():
        if app == "JupyterHub":
            cfg.pop("proxy_auth_token", None)
            cfg.pop("cookie_secret", None)
            cfg.pop("services", None)
        elif app == "ConfigurableHTTPProxy":
            cfg.pop("auth_token", None)
        elif app == "CryptKeeper":
            cfg.pop("keys", None)
        c[app].update(cfg)
  
    # load /usr/local/etc/jupyterhub/jupyterhub_config.d config files
    config_dir = "/usr/local/etc/jupyterhub/jupyterhub_config.d"
    if os.path.isdir(config_dir):
        for file_path in sorted(glob.glob(f"{config_dir}/*.py")):
            file_name = os.path.basename(file_path)
            print(f"Loading {config_dir} config: {file_name}")
            with open(file_path) as f:
                file_content = f.read()
            # compiling makes debugging easier: https://stackoverflow.com/a/437857
            exec(compile(source=file_content, filename=file_name, mode="exec"))
  
    # execute hub.extraConfig entries
    for key, config_py in sorted(get_config("hub.extraConfig", {}).items()):
        print(f"Loading extra config: {key}")
        exec(config_py)
  z2jh.py: |
    """
    Utility methods for use in jupyterhub_config.py and dynamic subconfigs.
  
    Methods here can be imported by extraConfig in values.yaml
    """
    import os
    from collections.abc import Mapping
    from functools import lru_cache
  
    import yaml
  
  
    # memoize so we only load config once
    @lru_cache
    def _load_config():
        """Load the Helm chart configuration used to render the Helm templates of
        the chart from a mounted k8s Secret, and merge in values from an optionally
        mounted secret (hub.existingSecret)."""
  
        cfg = {}
        for source in ("secret/values.yaml", "existing-secret/values.yaml"):
            path = f"/usr/local/etc/jupyterhub/{source}"
            if os.path.exists(path):
                print(f"Loading {path}")
                with open(path) as f:
                    values = yaml.safe_load(f)
                cfg = _merge_dictionaries(cfg, values)
            else:
                print(f"No config at {path}")
        return cfg
  
  
    @lru_cache
    def _get_config_value(key):
        """Load value from the k8s ConfigMap given a key."""
  
        path = f"/usr/local/etc/jupyterhub/config/{key}"
        if os.path.exists(path):
            with open(path) as f:
                return f.read()
        else:
            raise Exception(f"{path} not found!")
  
  
    @lru_cache
    def get_secret_value(key, default="never-explicitly-set"):
        """Load value from the user managed k8s Secret or the default k8s Secret
        given a key."""
  
        for source in ("existing-secret", "secret"):
            path = f"/usr/local/etc/jupyterhub/{source}/{key}"
            if os.path.exists(path):
                with open(path) as f:
                    return f.read()
        if default != "never-explicitly-set":
            return default
        raise Exception(f"{key} not found in either k8s Secret!")
  
  
    def get_name(name):
        """Returns the fullname of a resource given its short name"""
        return _get_config_value(name)
  
  
    def get_name_env(name, suffix=""):
        """Returns the fullname of a resource given its short name along with a
        suffix, converted to uppercase with dashes replaced with underscores. This
        is useful to reference named services associated environment variables, such
        as PROXY_PUBLIC_SERVICE_PORT."""
        env_key = _get_config_value(name) + suffix
        env_key = env_key.upper().replace("-", "_")
        return os.environ[env_key]
  
  
    def _merge_dictionaries(a, b):
        """Merge two dictionaries recursively.
  
        Simplified From https://stackoverflow.com/a/7205107
        """
        merged = a.copy()
        for key in b:
            if key in a:
                if isinstance(a[key], Mapping) and isinstance(b[key], Mapping):
                    merged[key] = _merge_dictionaries(a[key], b[key])
                else:
                    merged[key] = b[key]
            else:
                merged[key] = b[key]
        return merged
  
  
    def get_config(key, default=None):
        """
        Find a config item of a given name & return it
  
        Parses everything as YAML, so lists and dicts are available too
  
        get_config("a.b.c") returns config['a']['b']['c']
        """
        value = _load_config()
        # resolve path in yaml
        for level in key.split("."):
            if not isinstance(value, dict):
                # a parent is a scalar or null,
                # can't resolve full path
                return default
            if level not in value:
                return default
            else:
                value = value[level]
        return value
  
  
    def set_config_if_not_none(cparent, name, key):
        """
        Find a config item of a given name, set the corresponding Jupyter
        configuration item if not None
        """
        data = get_config(key)
        if data is not None:
            setattr(cparent, name, data)
  checksum_hook-image-puller: "e7e60844234e9c740573c1e6e8e13cca1968a61aafad4b86090ddb430207e943"
---
# Source: jupyterhub/templates/scheduling/user-scheduler/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
data:
  config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    leaderElection:
      resourceLock: endpointsleases
      resourceName: user-scheduler-lock
      resourceNamespace: "jupyterhub"
    profiles:
      - schedulerName: jupyterhub-user-scheduler
        plugins:
          score:
            disabled:
            - name: NodeResourcesBalancedAllocation
            - name: NodeAffinity
            - name: InterPodAffinity
            - name: NodeResourcesFit
            - name: ImageLocality
            enabled:
            - name: NodeAffinity
              weight: 14631
            - name: InterPodAffinity
              weight: 1331
            - name: NodeResourcesFit
              weight: 121
            - name: ImageLocality
              weight: 11
        pluginConfig:
          - args:
              scoringStrategy:
                resources:
                - name: cpu
                  weight: 1
                - name: memory
                  weight: 1
                type: MostAllocated
            name: NodeResourcesFit
---
# Source: jupyterhub/templates/hub/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: hub-db-dir
  labels:
    component: hub
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "1Gi"
---
# Source: jupyterhub/templates/scheduling/user-scheduler/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: jupyterhub-user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
rules:
  # Copied from the system:kube-scheduler ClusterRole of the k8s version
  # matching the kube-scheduler binary we use. A modification has been made to
  # resourceName fields to remain relevant for how we have named our resources
  # in this Helm chart.
  #
  # NOTE: These rules have been:
  #       - unchanged between 1.12 and 1.15
  #       - changed in 1.16
  #       - changed in 1.17
  #       - unchanged between 1.18 and 1.20
  #       - changed in 1.21: get/list/watch permission for namespace,
  #                          csidrivers, csistoragecapacities was added.
  #       - unchanged between 1.22 and 1.27
  #
  # ref: https://github.com/kubernetes/kubernetes/blob/v1.27.0/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/cluster-roles.yaml#L736-L892
  - apiGroups:
    - ""
    - events.k8s.io
    resources:
    - events
    verbs:
    - create
    - patch
    - update
  - apiGroups:
    - coordination.k8s.io
    resources:
    - leases
    verbs:
    - create
  - apiGroups:
    - coordination.k8s.io
    resourceNames:
    - user-scheduler-lock
    resources:
    - leases
    verbs:
    - get
    - update
  - apiGroups:
    - ""
    resources:
    - endpoints
    verbs:
    - create
  - apiGroups:
    - ""
    resourceNames:
    - user-scheduler-lock
    resources:
    - endpoints
    verbs:
    - get
    - update
  - apiGroups:
    - ""
    resources:
    - nodes
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    verbs:
    - delete
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - bindings
    - pods/binding
    verbs:
    - create
  - apiGroups:
    - ""
    resources:
    - pods/status
    verbs:
    - patch
    - update
  - apiGroups:
    - ""
    resources:
    - replicationcontrollers
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - statefulsets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - policy
    resources:
    - poddisruptionbudgets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - persistentvolumeclaims
    - persistentvolumes
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - authentication.k8s.io
    resources:
    - tokenreviews
    verbs:
    - create
  - apiGroups:
    - authorization.k8s.io
    resources:
    - subjectaccessreviews
    verbs:
    - create
  - apiGroups:
    - storage.k8s.io
    resources:
    - csinodes
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - namespaces
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - storage.k8s.io
    resources:
    - csidrivers
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - storage.k8s.io
    resources:
    - csistoragecapacities
    verbs:
    - get
    - list
    - watch

  # Copied from the system:volume-scheduler ClusterRole of the k8s version
  # matching the kube-scheduler binary we use.
  #
  # NOTE: These rules have not changed between 1.12 and 1.27.
  #
  # ref: https://github.com/kubernetes/kubernetes/blob/v1.27.0/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/cluster-roles.yaml#L1311-L1338
  - apiGroups:
    - ""
    resources:
    - persistentvolumes
    verbs:
    - get
    - list
    - patch
    - update
    - watch
  - apiGroups:
    - storage.k8s.io
    resources:
    - storageclasses
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - persistentvolumeclaims
    verbs:
    - get
    - list
    - patch
    - update
    - watch
---
# Source: jupyterhub/templates/scheduling/user-scheduler/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: jupyterhub-user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: user-scheduler
    namespace: "jupyterhub"
roleRef:
  kind: ClusterRole
  name: jupyterhub-user-scheduler
  apiGroup: rbac.authorization.k8s.io
---
# Source: jupyterhub/templates/hub/rbac.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
rules:
  - apiGroups: [""]       # "" indicates the core API group
    resources: ["pods", "persistentvolumeclaims", "secrets", "services"]
    verbs: ["get", "watch", "list", "create", "delete"]
  - apiGroups: [""]       # "" indicates the core API group
    resources: ["events"]
    verbs: ["get", "watch", "list"]
---
# Source: jupyterhub/templates/hub/rbac.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: hub
    namespace: "jupyterhub"
roleRef:
  kind: Role
  name: hub
  apiGroup: rbac.authorization.k8s.io
---
# Source: jupyterhub/templates/hub/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: /hub/metrics
    prometheus.io/port: "8081"
spec:
  type: ClusterIP
  selector:
    component: hub
    app: jupyterhub
    release: jupyterhub
  ports:
    - name: hub
      port: 8081
      targetPort: http
---
# Source: jupyterhub/templates/proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: proxy-api
  labels:
    component: proxy-api
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  selector:
    component: proxy
    app: jupyterhub
    release: jupyterhub
  ports:
    - port: 8001
      targetPort: api
---
# Source: jupyterhub/templates/proxy/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: proxy-public
  labels:
    component: proxy-public
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  selector:
    # This service will target the autohttps pod if autohttps is configured, and
    # the proxy pod if not. When autohttps is configured, the service proxy-http
    # will be around to target the proxy pod directly.
    component: proxy
    app: jupyterhub
    release: jupyterhub
  ports:
    - name: https
      port: 443
      # When HTTPS termination is handled outside our helm chart, pass traffic
      # coming in via this Service's port 443 to targeted pod's port meant for
      # HTTP traffic.
      targetPort: http
    - name: http
      port: 80
      targetPort: http
  type: ClusterIP
---
# Source: jupyterhub/templates/image-puller/daemonset-continuous.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: continuous-image-puller
  labels:
    component: continuous-image-puller
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  selector:
    matchLabels:
      component: continuous-image-puller
      app: jupyterhub
      release: jupyterhub
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 100%
  template:
    metadata:
      labels:
        component: continuous-image-puller
        app: jupyterhub
        release: jupyterhub
    spec:
      nodeSelector:
        cloud.google.com/gke-nodepool: ephemeral-pool
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
        - effect: NoSchedule
          key: ephemeral
          operator: Equal
          value: "true"
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      initContainers:
        - name: image-pull-singleuser
          image: jupyterhub/k8s-singleuser-sample:3.0.0-0.dev.git.6167.ha69d8ec6
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.9
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
---
# Source: jupyterhub/templates/hub/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hub
  labels:
    component: hub
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: hub
      app: jupyterhub
      release: jupyterhub
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        component: hub
        app: jupyterhub
        release: jupyterhub
        hub.jupyter.org/network-access-proxy-api: "true"
        hub.jupyter.org/network-access-proxy-http: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        checksum/config-map: 021468e956a0ec0bedaa8d1f3b8a6230a097597924aedd1a7565c546aec10ced
        checksum/secret: 04604e356aa857c06375583bb118278595d08938c560a1cedc2e042c72e812c6
    spec:
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      volumes:
        - name: config
          configMap:
            name: hub
        - name: secret
          secret:
            secretName: hub
        - name: files
          secret:
            secretName: hub
            items:
              - key: "bind_url_config"
                path: "bind_url_config"
              - key: "custom_kube_spawner"
                path: "custom_kube_spawner"
              - key: "keycloak_authenticator"
                path: "keycloak_authenticator"
              - key: "keycloak_config"
                path: "keycloak_config"
              - key: "spawner_config"
                path: "spawner_config"
        - name: pvc
          persistentVolumeClaim:
            claimName: hub-db-dir
      serviceAccountName: hub
      securityContext:
        fsGroup: 1000
      containers:
        - name: hub
          image: jupyterhub/k8s-hub:3.0.0-0.dev.git.6167.h43af1c55
          args:
            - jupyterhub
            - --config
            - /usr/local/etc/jupyterhub/jupyterhub_config.py
            - --upgrade-db
          volumeMounts:
            - mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.py
              subPath: jupyterhub_config.py
              name: config
            - mountPath: /usr/local/etc/jupyterhub/z2jh.py
              subPath: z2jh.py
              name: config
            - mountPath: /usr/local/etc/jupyterhub/config/
              name: config
            - mountPath: /usr/local/etc/jupyterhub/secret/
              name: secret
            - mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/00_bind_url_config.py
              subPath: "bind_url_config"
              name: files
            - mountPath: /app/custom/custom_kube_spawner.py
              subPath: "custom_kube_spawner"
              name: files
            - mountPath: /app/custom/keycloak_authenticator.py
              subPath: "keycloak_authenticator"
              name: files
            - mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/01_keycloak_config.py
              subPath: "keycloak_config"
              name: files
            - mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/02_spawner_config.py
              subPath: "spawner_config"
              name: files
            - mountPath: /srv/jupyterhub
              name: pvc
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 1000
            runAsUser: 1000
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: HELM_RELEASE_NAME
              value: "jupyterhub"
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hub
                  key: hub.config.ConfigurableHTTPProxy.auth_token
            - name: OAUTH2_AUTHORIZE_URL
              valueFrom:
                secretKeyRef:
                  key: OAUTH2_AUTHORIZE_URL
                  name: jupyterhub-oauth-keycloak
            - name: OAUTH2_TOKEN_URL
              valueFrom:
                secretKeyRef:
                  key: OAUTH2_TOKEN_URL
                  name: jupyterhub-oauth-keycloak
            - name: OAUTH2_USERDATA_URL
              valueFrom:
                secretKeyRef:
                  key: OAUTH2_USERDATA_URL
                  name: jupyterhub-oauth-keycloak
            - name: OAUTH_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  key: OAUTH_CLIENT_ID
                  name: jupyterhub-oauth-keycloak
            - name: OAUTH_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  key: OAUTH_CLIENT_SECRET
                  name: jupyterhub-oauth-keycloak
            - name: OAUTH_LOGOUT_REDIRECT_URL
              valueFrom:
                secretKeyRef:
                  key: OAUTH_LOGOUT_REDIRECT_URL
                  name: jupyterhub-oauth-keycloak
            - name: "PYTHONPATH"
              value: "/app/custom:$(PYTHONPATH)"
          ports:
            - name: http
              containerPort: 8081
          livenessProbe:
            initialDelaySeconds: 300
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 30
            httpGet:
              path: /hub/health
              port: http
          readinessProbe:
            initialDelaySeconds: 0
            periodSeconds: 2
            timeoutSeconds: 1
            failureThreshold: 1000
            httpGet:
              path: /hub/health
              port: http
---
# Source: jupyterhub/templates/proxy/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: proxy
  labels:
    component: proxy
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: proxy
      app: jupyterhub
      release: jupyterhub
  strategy:
    rollingUpdate: null
    type: Recreate
  template:
    metadata:
      labels:
        component: proxy
        app: jupyterhub
        release: jupyterhub
        hub.jupyter.org/network-access-hub: "true"
        hub.jupyter.org/network-access-singleuser: "true"
      annotations:
        # We want to restart proxy only if the auth token changes
        # Other changes to the hub config should not restart.
        # We truncate to 4 chars to avoid leaking auth token info,
        # since someone could brute force the hash to obtain the token
        #
        # Note that if auth_token has to be generated at random, it will be
        # generated at random here separately from being generated at random in
        # the k8s Secret template. This will cause this annotation to change to
        # match the k8s Secret during the first upgrade following an auth_token
        # was generated.
        checksum/auth-token: "81ad"
        checksum/proxy-secret: "01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b"
    spec:
      terminationGracePeriodSeconds: 60
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      containers:
        - name: chp
          image: jupyterhub/configurable-http-proxy:4.5.5
          command:
            - configurable-http-proxy
            - "--ip="
            - "--api-ip="
            - --api-port=8001
            - --default-target=http://hub:$(HUB_SERVICE_PORT)
            - --error-target=http://hub:$(HUB_SERVICE_PORT)/hub/error
            - --port=8000
          env:
            - name: CONFIGPROXY_AUTH_TOKEN
              valueFrom:
                secretKeyRef:
                  # NOTE: References the chart managed k8s Secret even if
                  #       hub.existingSecret is specified to avoid using the
                  #       lookup function on the user managed k8s Secret.
                  name: hub
                  key: hub.config.ConfigurableHTTPProxy.auth_token
          ports:
            - name: http
              containerPort: 8000
            - name: api
              containerPort: 8001
          livenessProbe:
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 30
            httpGet:
              path: /_chp_healthz
              port: http
              scheme: HTTP
          readinessProbe:
            initialDelaySeconds: 0
            periodSeconds: 2
            timeoutSeconds: 1
            failureThreshold: 1000
            httpGet:
              path: /_chp_healthz
              port: http
              scheme: HTTP
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
---
# Source: jupyterhub/templates/scheduling/user-scheduler/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-scheduler
  labels:
    component: user-scheduler
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      component: user-scheduler
      app: jupyterhub
      release: jupyterhub
  template:
    metadata:
      labels:
        component: user-scheduler
        app: jupyterhub
        release: jupyterhub
      annotations:
        checksum/config-map: 7d823537f32745283a6b82a1546d9cc51cda0f7bbe592de3570dabaf1e837b6a
    spec:
      
      serviceAccountName: user-scheduler
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [core]
      volumes:
        - name: config
          configMap:
            name: user-scheduler
      containers:
        - name: kube-scheduler
          image: registry.k8s.io/kube-scheduler:v1.26.5
          command:
            - /usr/local/bin/kube-scheduler
            # NOTE: --authentication-skip-lookup=true is used to avoid a
            #       seemingly harmless error, if we need to not skip
            #       "authentication lookup" in the future, see the linked issue.
            #
            # ref: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/1894
            - --config=/etc/user-scheduler/config.yaml
            - --authentication-skip-lookup=true
            - --v=4
          volumeMounts:
            - mountPath: /etc/user-scheduler
              name: config
          livenessProbe:
            httpGet:
              path: /healthz
              scheme: HTTPS
              port: 10259
            initialDelaySeconds: 15
          readinessProbe:
            httpGet:
              path: /healthz
              scheme: HTTPS
              port: 10259
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
---
# Source: jupyterhub/templates/scheduling/user-placeholder/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: user-placeholder
  labels:
    component: user-placeholder
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
spec:
  podManagementPolicy: Parallel
  replicas: 0
  selector:
    matchLabels:
      component: user-placeholder
      app: jupyterhub
      release: jupyterhub
  serviceName: user-placeholder
  template:
    metadata:
      labels:
        component: user-placeholder
        app: jupyterhub
        release: jupyterhub
    spec:
      schedulerName: jupyterhub-user-scheduler
      nodeSelector:
        cloud.google.com/gke-nodepool: ephemeral-pool
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
        - effect: NoSchedule
          key: ephemeral
          operator: Equal
          value: "true"
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: hub.jupyter.org/node-purpose
                    operator: In
                    values: [user]
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.9
          resources:
            requests:
              memory: 1G
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
---
# Source: jupyterhub/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jupyterhub
  labels:
    component: ingress
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  ingressClassName: "nginx"
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: proxy-public
                port:
                  name: http
      host: "jupyterhub.prod.k8s.86labs.com"
  tls:
    - hosts:
      - jupyterhub.prod.k8s.86labs.com
      secretName: jupyterhub-prod-k8s-86labs-com
---
# Source: jupyterhub/templates/image-puller/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "0"
---
# Source: jupyterhub/templates/image-puller/rbac.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "0"
rules:
  - apiGroups: ["apps"]       # "" indicates the core API group
    resources: ["daemonsets"]
    verbs: ["get"]
---
# Source: jupyterhub/templates/image-puller/rbac.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "0"
subjects:
  - kind: ServiceAccount
    name: hook-image-awaiter
    namespace: "jupyterhub"
roleRef:
  kind: Role
  name: hook-image-awaiter
  apiGroup: rbac.authorization.k8s.io
---
# Source: jupyterhub/templates/image-puller/daemonset-hook.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: hook-image-puller
  labels:
    component: hook-image-puller
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "-10"
spec:
  selector:
    matchLabels:
      component: hook-image-puller
      app: jupyterhub
      release: jupyterhub
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 100%
  template:
    metadata:
      labels:
        component: hook-image-puller
        app: jupyterhub
        release: jupyterhub
    spec:
      nodeSelector:
        cloud.google.com/gke-nodepool: ephemeral-pool
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: user
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: user
        - effect: NoSchedule
          key: ephemeral
          operator: Equal
          value: "true"
      terminationGracePeriodSeconds: 0
      automountServiceAccountToken: false
      initContainers:
        - name: image-pull-singleuser
          image: jupyterhub/k8s-singleuser-sample:3.0.0-0.dev.git.6167.ha69d8ec6
          command:
            - /bin/sh
            - -c
            - echo "Pulling complete"
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
      containers:
        - name: pause
          image: registry.k8s.io/pause:3.9
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
---
# Source: jupyterhub/templates/image-puller/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hook-image-awaiter
  labels:
    component: image-puller
    app: jupyterhub
    release: jupyterhub
    chart: jupyterhub-3.0.0-0.dev.git.6171.h0f4b2d0b
    heritage: Helm
    hub.jupyter.org/deletable: "true"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "10"
spec:
  template:
    # The hook-image-awaiter Job and hook-image-puller DaemonSet was
    # conditionally created based on this state:
    #
    # prePuller.hook.enabled=true
    # prePuller.hook.pullOnlyOnChanges=true
    # post-upgrade checksum != pre-upgrade checksum (of the hook-image-puller DaemonSet)
    # "e7e60844234e9c740573c1e6e8e13cca1968a61aafad4b86090ddb430207e943" != ""
    #
    metadata:
      labels:
        component: image-puller
        app: jupyterhub
        release: jupyterhub
    spec:
      restartPolicy: Never
      serviceAccountName: hook-image-awaiter
      tolerations:
        - effect: NoSchedule
          key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
        - effect: NoSchedule
          key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
      containers:
        - image: jupyterhub/k8s-image-awaiter:3.0.0-0.dev.git.6114.hc0d0b3b0
          name: hook-image-awaiter
          command:
            - /image-awaiter
            - -ca-path=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            - -auth-token-path=/var/run/secrets/kubernetes.io/serviceaccount/token
            - -api-server-address=https://kubernetes.default.svc:$(KUBERNETES_SERVICE_PORT)
            - -namespace=jupyterhub
            - -daemonset=hook-image-puller
            - -pod-scheduling-wait-duration=10
          securityContext:
            allowPrivilegeEscalation: false
            runAsGroup: 65534
            runAsUser: 65534
